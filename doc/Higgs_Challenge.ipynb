{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Higgs Boson ML Challenge using MLaaS4HEP"
      ],
      "metadata": {
        "id": "c7g5mPIRl0Ik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Install all requirements for MLaaS4HEP and to tackle the challenge\n",
        "\n"
      ],
      "metadata": {
        "id": "_o0g-r9GVt-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib notebook\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "4lxyCerwAWp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uproot\n",
        "!pip install awkward"
      ],
      "metadata": {
        "id": "GiPlzLf0MPa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Installing ROOT"
      ],
      "metadata": {
        "id": "v-V2BLamUg6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/palamatt95/HEP-ML/releases/download/ROOT/ROOT.tar.zip\n",
        "!unzip /content/ROOT.tar.zip\n",
        "!tar -xf  ROOT.tar\n",
        "!apt-get install git dpkg-dev cmake g++ gcc binutils libx11-dev libxpm-dev libxft-dev libxext-dev tar gfortran subversion\n",
        "import sys\n",
        "sys.path.append(\"/content/root_build/\")\n",
        "sys.path.append(\"/content/root_build/bin/\")\n",
        "sys.path.append(\"/content/root_build/include/\")\n",
        "sys.path.append(\"/content/root_build/lib/\")\n",
        "import ctypes\n",
        "ctypes.cdll.LoadLibrary('/content/root_build/lib//libCore.so')\n",
        "ctypes.cdll.LoadLibrary('/content/root_build/lib//libThread.so')\n",
        "ctypes.cdll.LoadLibrary('/content/root_build/lib//libTreePlayer.so')"
      ],
      "metadata": {
        "id": "cDayW05A_uJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Â Before starting, follow the below steps.\n"
      ],
      "metadata": {
        "id": "SCBlF4mFUYAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Create your Kaggle API Token\n",
        "\n",
        "- Go to your Kaggle profile and click on Edit Public Profile.\n",
        "- Scroll the page until API section and click on Create New API Token button.\n",
        "- A file named kaggle.json will get downloaded"
      ],
      "metadata": {
        "id": "el3UAULwxs7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Upload kaggle.json to Google Drive\n",
        "\n",
        "- Create a folder named \"Kaggle\" in your Google Drive\n",
        "- Upload your downloaded kaggle.json file to the created folder"
      ],
      "metadata": {
        "id": "KUobAXU_yZbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we proceed with the download of the dataset in our Google Drive"
      ],
      "metadata": {
        "id": "mrGXQZ7Mzcns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2Nw5dJuPDEQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/Kaggle\""
      ],
      "metadata": {
        "id": "Dacs4fKTPl-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Kaggle/"
      ],
      "metadata": {
        "id": "W2hHN7UHQLsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c higgs-boson"
      ],
      "metadata": {
        "id": "QT7oKjRvQR13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "for file in os.listdir():\n",
        "    if file.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(file, \"r\") as zip_file:\n",
        "            zip_file.extractall()\n",
        "        os.remove(file)"
      ],
      "metadata": {
        "id": "rvW4WmQfRZPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in os.listdir():\n",
        "    if file.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(file, \"r\") as zip_file:\n",
        "            zip_file.extractall()\n",
        "        os.remove(file)"
      ],
      "metadata": {
        "id": "O4EdN5EfSpYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing all the required Libraries**\n",
        "\n",
        "> As you can see, a function \"save_fig\" has been defined that will allow you to save the images that will be generated"
      ],
      "metadata": {
        "id": "BUydz4b3ghe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn import metrics\n",
        "from tensorflow import keras\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"Images\")\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"pdf\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "metadata": {
        "id": "NKBy1W8bggqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We start by opening the file containing the training set and inspecting it"
      ],
      "metadata": {
        "id": "WUgfuLzMZY8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training = pd.read_csv(\"/content/drive/MyDrive/Kaggle/training.csv\")"
      ],
      "metadata": {
        "id": "Zs57wPwRVZnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training.columns.tolist())\n",
        "print(len(training.columns.tolist()))\n",
        "training.shape"
      ],
      "metadata": {
        "id": "iUVF-ii4V_YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Weights and EventId from the features of the training dataset**"
      ],
      "metadata": {
        "id": "0UqgiAPkZ2Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training.drop(['EventId', 'Weight'], inplace=True, axis=1)\n",
        "training.shape"
      ],
      "metadata": {
        "id": "0NvkUrvEZoaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting the labels: Signal (s) = 1 and Backgroung (b) = 0**"
      ],
      "metadata": {
        "id": "I-s_Ss51cEQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training.loc[training[\"Label\"] == \"s\", \"Label\"] = 1\n",
        "training.loc[training[\"Label\"] == \"b\", \"Label\"] = 0\n",
        "training[\"Label\"].value_counts()\n",
        "training.head()"
      ],
      "metadata": {
        "id": "rKifawd5aHvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> And split the Dataset in two part: one containing signal events and the other one containing background"
      ],
      "metadata": {
        "id": "5hjTpbAWmkjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_sig = training[training['Label'] == 1]\n",
        "print(len(training_sig))\n",
        "training_bkg = training[training['Label'] == 0]\n",
        "print(len(training_bkg))"
      ],
      "metadata": {
        "id": "Ic6hv7AQbxR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training"
      ],
      "metadata": {
        "id": "Pyu4pDnx2b6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will check the shape of each variable in the Training Set"
      ],
      "metadata": {
        "id": "9-2hwt1vDHyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = [30,30]\n",
        "training.hist()\n",
        "plt.show()\n",
        "save_fig('feature_hist')"
      ],
      "metadata": {
        "id": "k9gMHPtTBVb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams[\"figure.figsize\"] = [10,10]\n",
        "training[\"Label\"].hist()\n",
        "plt.show()\n",
        "save_fig('label_hist')"
      ],
      "metadata": {
        "id": "CjDns1jG6nJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Look for correlation between variables using the Correlation Matrix**\n",
        "\n"
      ],
      "metadata": {
        "id": "e0rIzHAZDX7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = training.corr()"
      ],
      "metadata": {
        "id": "s--vrr5EDChV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(30,15))\n",
        "ax = sn.heatmap(\n",
        "    corr_matrix, \n",
        "    vmin=-1, vmax=1, center=0,\n",
        "    cmap=sn.diverging_palette(20, 220, n=200),\n",
        "    square=True\n",
        ")\n",
        "\n",
        "ax.set_yticklabels(ax.get_yticklabels(which='major'), fontsize=20)\n",
        "ax.set_xticklabels(\n",
        "    ax.get_xticklabels(which='major'),\n",
        "    fontsize = 20,\n",
        "    rotation=90,\n",
        "    horizontalalignment='center'\n",
        ");\n",
        "cbar = ax.collections[0].colorbar\n",
        "cbar.ax.tick_params(labelsize=30)\n",
        "save_fig('correlation_matrix')"
      ],
      "metadata": {
        "id": "nJGjBiTfDUuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Highly Correlated variables should be removed**\n",
        "> When the correlation is strong, highly correlated variables do not convey extra information. The following list shows these variables, which will later be removed.\n",
        "- DER_lep_eta_centrality is highly correlated with DER_prodeta_jet_jet\n",
        "- DER_mass_jet_jet highly correlated with DER_deltaeta_jet_jet\n",
        "- DER_prodeta_jet_jet is highly correlated with DER_mass_jet_jet\n",
        "- PRI_jet_all_pt is highly correlated with DER_sum_pt\n",
        "- PRI_jet_leading_eta is highly correlated with PRI_jet_leading_pt\n",
        "- PRI_jet_leading_phi is highly correlated with PRI_jet_leading_eta\n",
        "- PRI_jet_subleading_eta is highly correlated with PRI_jet_subleading_pt\n",
        "- PRI_jet_subleading_phi is highly correlated with PRI_jet_subleading_eta\n",
        "- PRI_jet_subleading_pt is highly correlated with DER_lep_eta_centrality\n",
        "- PRI_met_sumet is highly correlated with DER_sum_pt"
      ],
      "metadata": {
        "id": "P8tcqSbfEUIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Log-transformation makes visualization better when dealing with skewed variables**\n",
        "\n",
        "> Log-transformation makes our skewed original data more normal and it improves linearity between our dependent and independent variables."
      ],
      "metadata": {
        "id": "eO9EaUMRFRLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "\n",
        "select=training[[\"DER_sum_pt\",\"PRI_met_sumet\",\"Label\"]]\n",
        "sn.pairplot(select, hue=\"Label\").fig.suptitle('Without Log Transformation', y=1.05)\n",
        "plt.show()\n",
        "\n",
        "x = select[\"DER_sum_pt\"].apply(np.log)\n",
        "y = select[\"PRI_met_sumet\"].apply(np.log)\n",
        "z= select[\"Label\"]\n",
        "\n",
        "d = {'DER_sum_pt': x, 'PRI_met_sumet': y,'Label':z}\n",
        "new_ = pd.DataFrame(d)\n",
        "sn.pairplot(new_, hue=\"Label\").fig.suptitle('With Log Transformation', y=1.05)\n",
        "plt.show()\n",
        "save_fig('log_visualisation')\n"
      ],
      "metadata": {
        "id": "NUzAKG5FDugg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Below there is a list of variables that could be log-transformed.\n",
        "- DER_sum_pt\n",
        "- PRI_met_sumet\n",
        "- PRI_jet_all_pt\n",
        "- PRI_jet_subleading_pt\n",
        "- PRI_jet_leading_pt\n",
        "- PRI_lep_pt\n",
        "- PRI_tau_pt\n",
        "- DER_mass_jet_jet"
      ],
      "metadata": {
        "id": "9v4sWM7WhMxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Importance using XGBoost**\n",
        "\n",
        "> Now the study of feature importance will be carried out. To use this technique, it was decided to use an XGBoost classifier"
      ],
      "metadata": {
        "id": "TpS6sOKGhw5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = training.iloc[:,:-1], training.iloc[:,-1]"
      ],
      "metadata": {
        "id": "1Xnc5j4ChaU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model = xgb.XGBClassifier()\n",
        "model.fit(X,y)"
      ],
      "metadata": {
        "id": "NqvfE-9ShmZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "plt.figure()\n",
        "plt.rcParams[\"figure.figsize\"] = [20,10]\n",
        "plt.barh(X.columns[indices], importances[indices], align=\"center\")\n",
        "\n",
        "plt.yticks(fontsize=18)\n",
        "plt.ylim([-1, X.shape[1]])\n",
        "plt.xticks(fontsize=20)\n",
        "plt.show()\n",
        "save_fig('feature_importance')\n"
      ],
      "metadata": {
        "id": "BiLFLZi3iDOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> As shown in the image above, from the feature importance it can be seen that the importance of variables whose name ends with \"-phi\" is almost zero and therefore they should be removed>"
      ],
      "metadata": {
        "id": "fCTkt40OinnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Missing Values**\n",
        "\n",
        "> It can be seen that there are \"out of range\" values in the dataset that have been set to -999.0. it is necessary, before using MLaaS4HEP, to handle these values.\n",
        "Our chosen approach is to replace these missing values with the median as shown below."
      ],
      "metadata": {
        "id": "LFuMLrNPkLXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_sig.replace(-999.000, np.nan, inplace=True)\n",
        "training_bkg.replace(-999.000, np.nan, inplace=True)"
      ],
      "metadata": {
        "id": "BsULUaDxiHB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sig = training_sig.fillna(training_sig.median())\n",
        "training_bkg = training_bkg.fillna(training_bkg.median())"
      ],
      "metadata": {
        "id": "JEdteqxakfYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conversion to CSV**\n",
        "\n",
        "> We will now consider two datasets, one referring to \"signal\" events and one referring to \"background\" events, from which the \"Label\" feature will be removed and that will be converted in CSV format."
      ],
      "metadata": {
        "id": "0wcJQOt7j4t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_sig.drop(['Label'], inplace=True, axis=1)\n",
        "training_bkg.drop(['Label'], inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "ns5lvZ6hnKrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sig.to_csv(\"training_s.csv\", index=False)\n",
        "training_bkg.to_csv(\"training_b.csv\", index=False)"
      ],
      "metadata": {
        "id": "WsUif0Rdk-us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "UTj7FMMXBEc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **From CSV to ROOT data format**\n",
        "\n",
        "> Since MLaaS4HEP requires the input data to be ROOT files, further conversion is required."
      ],
      "metadata": {
        "id": "Ni1A5BkvlQln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ROOT\n",
        "\n",
        "fileName = \"training_s.csv\";\n",
        "rdf = ROOT.RDF.MakeCsvDataFrame(fileName);\n",
        "rdf.Snapshot(\"myTree\", \"training_s.root\");\n",
        "\n",
        "fileName = \"training_b.csv\";\n",
        "rdf = ROOT.RDF.MakeCsvDataFrame(fileName);\n",
        "rdf.Snapshot(\"myTree\", \"training_b.root\");"
      ],
      "metadata": {
        "id": "csWXkbDSAZnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> With this last step completed, we can move on to use the MLaaS4HEP framework."
      ],
      "metadata": {
        "id": "yhFkHcHFnKD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLaaS4HEP\n",
        "\n",
        " First, it is necessary to do a \"git clone\" to download the code"
      ],
      "metadata": {
        "id": "MjQ6sqmVmtfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b Higgs_challenge https://github.com/lgiommi/MLaaS4HEP.git"
      ],
      "metadata": {
        "id": "7RBgC_kTPAji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "exK0JdPJWqlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Place ourselves in the folder containing the file workflow.py"
      ],
      "metadata": {
        "id": "HVg6yxlCnkgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MLaaS4HEP/src/python/MLaaS4HEP"
      ],
      "metadata": {
        "id": "hsPlg8viW72k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> And create a new folder containing the ROOT files that will be used for the challenge"
      ],
      "metadata": {
        "id": "1Put6-P_nziA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir challenge_data"
      ],
      "metadata": {
        "id": "HQE7eulBXCLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Kaggle"
      ],
      "metadata": {
        "id": "T5wNSr4JhRjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mv -t $PWD/MLaaS4HEP/src/python/MLaaS4HEP/challenge_data training_b.root training_s.root"
      ],
      "metadata": {
        "id": "jhS51Uq6hbNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Kaggle/MLaaS4HEP/src/python/MLaaS4HEP"
      ],
      "metadata": {
        "id": "u7zU6ClEhzCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLaaS4HEP Training Workflow\n",
        "The MLaaS4HEP Training Workflow is performed by running the workflow.py python\n",
        "script which takes several argument as input.\n",
        "\n"
      ],
      "metadata": {
        "id": "EiqwpDQk4pfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- files.txt: stores paths and names of the input ROOT files."
      ],
      "metadata": {
        "id": "Kt8JkSZdxU3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cat files.txt"
      ],
      "metadata": {
        "id": "tFW2xC4D5BDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- labels.txt contains the labels of the respective ROOT files, it is used for classification\n",
        "problems."
      ],
      "metadata": {
        "id": "84Lz5-uBxavI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cat labels.txt"
      ],
      "metadata": {
        "id": "TxRbJ1GS5KQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- a python file that contains the definition of the ML model chosen by user\n",
        "\n",
        "> To address this challenge, we choose four different models with which we will run MLaaS4HEP. The following is the definition of the first model, in which a gradient boosting classifier was defined."
      ],
      "metadata": {
        "id": "r-ZXYnq_xpGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cat gradient_boosting.py"
      ],
      "metadata": {
        "id": "Gz3Glq745KIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- params.json stores parameters on which MLaaS4HEP relies, such as chunk size,\n",
        "batch size, number of epochs and so on.\n",
        "\n",
        "> In our case, two different types of params.json are defined depending on the model used.\n",
        "\n",
        "> In addition, it can be seen that this file contains the names of the variables that we decided to remove from the dataset as suggested by the preliminary analysis and this can be done by entering the name of the variables as the value of the \"exclude_branches\" key"
      ],
      "metadata": {
        "id": "MLh44dGH0OkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cat params_DT.json"
      ],
      "metadata": {
        "id": "5_COhLg_5Jzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- preproc.json contains the preprocessing operations that the user wants to perform.\n",
        "\n",
        "> When we talk about preprocessing operations, we refer to operations\n",
        "that allow the users to manipulate data, i.e.:\n",
        "- new branches definition,\n",
        "- application of cuts on branches, both new and existing ones,\n",
        "- removal of branches that may not be useful for model training.\n",
        "\n",
        "> In this case, we use preproc.json to apply the logarithmic transformation on the features of the dataset as suggested in the previous analysis."
      ],
      "metadata": {
        "id": "mhpM33SMQQ8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cat preproc.json"
      ],
      "metadata": {
        "id": "S2XG4Rrb5Nps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we are ready to execute MLaaS4HEP"
      ],
      "metadata": {
        "id": "lP8cbHNpRYwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python workflow.py --files=files.txt --labels=labels.txt --model=gradient_boosting.py --params=params_DT.json --preproc=preproc.json --fout=challenge_data/GBModel.pkl"
      ],
      "metadata": {
        "id": "7BhX0DxuiE5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Definition of the XGBoost Classifier"
      ],
      "metadata": {
        "id": "046wBqpKReD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cat XGBoost.py"
      ],
      "metadata": {
        "id": "TJcy3G7s5i4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python workflow.py --files=files.txt --labels=labels.txt --model=XGBoost.py --params=params_DT.json --preproc=preproc.json --fout=challenge_data/XGBModel.json"
      ],
      "metadata": {
        "id": "YZH8COs0lFxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Definition of a Sequential Neural Network using Keras"
      ],
      "metadata": {
        "id": "v9ZBB9eWRiIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cat sequential_NN.py"
      ],
      "metadata": {
        "id": "6sLSwrJH5jfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> params_NN.json: this file contains different information than the previous one, such as chunk size and number of epochs"
      ],
      "metadata": {
        "id": "iWLvB-F5RqP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cat params_NN.json"
      ],
      "metadata": {
        "id": "uvH3aBbB5kww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python workflow.py --files=files.txt --labels=labels.txt --model=sequential_NN.py --params=params_NN.json --preproc=preproc.json --fout=challenge_data/KerasModel.h5"
      ],
      "metadata": {
        "id": "fBsNBl-CiNLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Definition of a Sequential Neural Network using PyTorch"
      ],
      "metadata": {
        "id": "bqhNv29aUZAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cat clf_torch.py"
      ],
      "metadata": {
        "id": "Dczb_lcAlAkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python workflow.py --files=files.txt --labels=labels.txt --model=clf_torch.py --params=params_NN.json --preproc=preproc.json --fout=challenge_data/torch_model.pth"
      ],
      "metadata": {
        "id": "UvjMsI1ZJxnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "5JUpPmqmstPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the Test set for predictions"
      ],
      "metadata": {
        "id": "u-OGxtbKsyoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Now we open the file containing the test set"
      ],
      "metadata": {
        "id": "8Vb_zgcbXf5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Kaggle"
      ],
      "metadata": {
        "id": "Au2o-2djWafK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_subset = pd.read_csv(\"/content/drive/MyDrive/Kaggle/test.csv\")"
      ],
      "metadata": {
        "id": "LI7tu34Us11p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_subset.info()"
      ],
      "metadata": {
        "id": "V67FJZy1Wy30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Replace missing values using the median (like in the previous case)"
      ],
      "metadata": {
        "id": "QZsSsDrPXr3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_subset.replace(-999.000, np.nan, inplace=True)"
      ],
      "metadata": {
        "id": "iKgSwznfXVfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test_subset.fillna(test_subset.median())"
      ],
      "metadata": {
        "id": "kCelJOHpXiPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "id": "vQazTx2bXo2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Set the variable \"EventId\" as the index of the Dataframe"
      ],
      "metadata": {
        "id": "55fCCxBeX5mK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = test['EventId']\n",
        "test.set_index(['EventId'],inplace = True)"
      ],
      "metadata": {
        "id": "fc5HCrBOX2iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Apply the log-transformation just as was done within MLaaS4HEP using the preproc.json file"
      ],
      "metadata": {
        "id": "hd4qy9sWYCmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logcolumns = [\"DER_sum_pt\",\"PRI_jet_leading_pt\",\"PRI_lep_pt\",\"PRI_tau_pt\"]\n",
        "test[logcolumns]"
      ],
      "metadata": {
        "id": "h2qmjk5FYDCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.loc[:, logcolumns] = np.log(test[logcolumns])\n",
        "test[logcolumns]"
      ],
      "metadata": {
        "id": "-k08xg9IaXy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "Q3S93m86eKNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Again, it is necessary to select the features to be excluded"
      ],
      "metadata": {
        "id": "ICE1rueAdyk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = test.drop([\"PRI_tau_phi\",\n",
        "        \"PRI_lep_phi\", \"PRI_met_phi\", \"PRI_jet_leading_phi\",\n",
        "        \"PRI_jet_subleading_phi\", \"DER_lep_eta_centrality\",\n",
        "        \"DER_mass_jet_jet\", \"DER_prodeta_jet_jet\", \"PRI_jet_all_pt\",\n",
        "        \"PRI_jet_leading_eta\", \"PRI_jet_subleading_eta\",\n",
        "        \"PRI_jet_subleading_pt\", \"PRI_met_sumet\"], axis=1)"
      ],
      "metadata": {
        "id": "sXaRZ5ICfa8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> And we conclude by putting the features in the same order as they were processed by MLaaS4HEP, which arranges them in alphabetical order. In case new variables are defined, as in our case, they will be placed after existing features"
      ],
      "metadata": {
        "id": "-vk19k6-d7p5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['DER_deltaeta_jet_jet','DER_deltar_tau_lep','DER_mass_MMC','DER_mass_transverse_met_lep','DER_mass_vis',\n",
        "       'DER_met_phi_centrality','DER_pt_h','DER_pt_ratio_lep_tau','DER_pt_tot','PRI_jet_num', 'PRI_lep_eta', \n",
        "        'PRI_met', 'PRI_tau_eta','DER_sum_pt','PRI_jet_leading_pt','PRI_lep_pt','PRI_tau_pt']\n",
        "\n",
        "\n",
        "len(cols)"
      ],
      "metadata": {
        "id": "BtmTRanFfatE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = x_test[cols]"
      ],
      "metadata": {
        "id": "GWIMDVnoelzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test"
      ],
      "metadata": {
        "id": "GgSpWAqUDYsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the submission file"
      ],
      "metadata": {
        "id": "CA8anvYGe2k7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> As a first step, it is necessary to use the models trained by MLaaS4HEP to make inference on the new data."
      ],
      "metadata": {
        "id": "SCquy7eNfIM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The pickle library allows to save and load trained Scikit Learn model"
      ],
      "metadata": {
        "id": "9QFx7VCigE0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_code(mfile, fname):\n",
        "    \"\"\"\n",
        "    Load function from given python module (file)\n",
        "    \"\"\"\n",
        "    mname = mfile.split('.py')[0].replace('/', '.')\n",
        "    try:\n",
        "        mod = __import__(mname, fromlist=['model'])\n",
        "        func = getattr(mod, fname)\n",
        "        #print(\"load {} {} {}\".format(mfile, func, func.__doc__))\n",
        "        return func\n",
        "    except ImportError:\n",
        "        traceback.print_exc()\n",
        "        msg = \"Please provide file name with 'def %s' implementation\" % fname\n",
        "        msg += \"\\nThe file should be available in PYTHONPATH\"\n",
        "        print(msg)\n",
        "        raise"
      ],
      "metadata": {
        "id": "6pdgvkHzIPbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Kaggle/MLaaS4HEP/src/python/MLaaS4HEP/"
      ],
      "metadata": {
        "id": "dxSD4ZkxFIMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PTModel = load_code('clf_torch.py', 'model')"
      ],
      "metadata": {
        "id": "ZvzKxKp72Ars"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idim = np.shape(x_test)[-1]\n",
        "PTmodel = PTModel(idim)"
      ],
      "metadata": {
        "id": "COqK2HGBGHIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd challenge_data/"
      ],
      "metadata": {
        "id": "zm8i3cE5L9iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PTmodel.load_state_dict(torch.load(\"torch_model.pth\"))\n",
        "PTmodel.eval()"
      ],
      "metadata": {
        "id": "FLA9Okd-2img"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KModel = keras.models.load_model(\"KerasModel.h5\")\n",
        "XGBModel = xgb.XGBClassifier()\n",
        "XGBModel.load_model('XGBModel.json')\n",
        "GBModel = pickle.load(open('GBModel.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "y5ddwBSrgoXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PTpred = PTmodel(x_test.values)\n",
        "PTpred = PTpred.detach().numpy()"
      ],
      "metadata": {
        "id": "U30nGd2isfRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kpred = KModel.predict(x_test.values)\n",
        "Xpred = XGBModel.predict_proba(x_test.values)[:,1]\n",
        "Gpred = GBModel.predict_proba(x_test.values)[:,1]"
      ],
      "metadata": {
        "id": "peKCsiztg5Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Now let's proceed with creating the CSV to make the submission to Kaggle, but first we need to define the threshold.\n",
        "\n",
        "> If we consider, for example, a Neural Network, the activation function connected to the output layer will map any variable in a range of values between 0 and 1. So, for binary classification problems, it is necessary to set a threshold which divides the values into two categories: values below the threshold are assigned to the 0 category otherwise to the 1 category.\n",
        "This threshold is usually set to 0.5 by default, but it may not always be the best solution.\n",
        "\n",
        "> At the end of the execution of MLaaS4HEP, for each run at the end of the output you can see a \"Best Threshold=...\": such value corresponds to the best threshold according to the predictions made on the training set. There is a threshold present for each model and it is necessary to enter the values below.\n",
        "The ones you see written here are the ones obtained from a previous launch.\n",
        "\n",
        "> Note that K stands for Keras and refers to the neural network, X refers to the XGBoost classifier and G refers to the gradient boosting classifier"
      ],
      "metadata": {
        "id": "fgUVJtKTg1jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PTthresh = 0.384685"
      ],
      "metadata": {
        "id": "UhFtCCJ95pjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kthresh = 0.224096\n",
        "Xthresh = 0.478793\n",
        "Gthresh = 0.394601"
      ],
      "metadata": {
        "id": "U8wa4MK8hJJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Here we are building the submission file for each model"
      ],
      "metadata": {
        "id": "UEEt0fYopg3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xp = np.empty(len(x_test.values), dtype=object)\n",
        "Xp[Xpred > Xthresh] = 's'\n",
        "Xp[Xpred <= Xthresh] = 'b'\n",
        "Xr = np.argsort(Xpred) + 1\n",
        "XGBsub = pd.DataFrame({\"EventId\": ids, \"RankOrder\": Xr, \"Class\": Xp})\n",
        "XGBsub"
      ],
      "metadata": {
        "id": "xktm61VXjdjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Gp = np.empty(len(x_test.values), dtype=object)\n",
        "Gp[Gpred > Gthresh] = 's'\n",
        "Gp[Gpred <= Gthresh] = 'b'\n",
        "Gr = np.argsort(Gpred) + 1\n",
        "GBsub = pd.DataFrame({\"EventId\": ids, \"RankOrder\": Gr, \"Class\": Gp})\n",
        "GBsub"
      ],
      "metadata": {
        "id": "cqE6eCwDkR8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kp = np.empty(len(x_test.values), dtype=object)\n",
        "Kpred = Kpred.flatten()\n",
        "Kp[Kpred > Kthresh] = 's'\n",
        "Kp[Kpred <= Kthresh] = 'b'\n",
        "Kr = np.argsort(Kpred) + 1\n",
        "Ksub = pd.DataFrame({\"EventId\": ids, \"RankOrder\": Kr, \"Class\": Kp})\n",
        "Ksub"
      ],
      "metadata": {
        "id": "A1m9tRlTn1Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PTp = np.empty(len(x_test.values), dtype=object)\n",
        "PTpred = PTpred.flatten()\n",
        "PTp[PTpred > PTthresh] = 's'\n",
        "PTp[PTpred <= PTthresh] = 'b'\n",
        "PTr = np.argsort(PTpred) + 1\n",
        "PTsub = pd.DataFrame({\"EventId\": ids, \"RankOrder\": PTr, \"Class\": PTp})\n",
        "PTsub"
      ],
      "metadata": {
        "id": "OCkzHwuN5x8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ksub.to_csv('submission_NN.csv', index=False)\n",
        "GBsub.to_csv('submission_GBC.csv', index=False)\n",
        "XGBsub.to_csv('submission_XGB.csv', index=False)"
      ],
      "metadata": {
        "id": "-eLnPdSiqhnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PTsub.to_csv('submission_PT.csv', index=False)"
      ],
      "metadata": {
        "id": "c9PvwUEOU-Oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> And finally we proceed to upload each submission file to the Kaggle site."
      ],
      "metadata": {
        "id": "RkiQNGQOprl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit higgs-boson -f submission_NN.csv -m \"Submission with a NN written in Keras\"\n",
        "!kaggle competitions submit higgs-boson -f submission_GBC.csv -m \"Submission with a Gradient Boosting Classifier\"\n",
        "!kaggle competitions submit higgs-boson -f submission_XGB.csv -m \"Submission with a XGBoost Classifier\""
      ],
      "metadata": {
        "id": "zBsJooUmHXUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit higgs-boson -f submission_PT.csv -m \"Submission with a PyTorch Classifier\""
      ],
      "metadata": {
        "id": "2ydl0WjcVBm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can go and check at https://www.kaggle.com/competitions/higgs-boson/leaderboard) what was the score obtained by the models."
      ],
      "metadata": {
        "id": "gouFCkD9qDQm"
      }
    }
  ]
}